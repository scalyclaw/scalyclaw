<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Memory — ScalyClaw Docs</title>
  <link rel="icon" type="image/svg+xml" href="../assets/logo.svg" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <div class="docs-layout">
    <main class="docs-main">
      <div class="docs-content">

        <h1>Memory</h1>
        <p>ScalyClaw remembers. Not just within a conversation, but across all conversations, all channels, forever. Every interaction is an opportunity to learn something about the user — a preference, a fact, a relationship, a recurring topic. That knowledge accumulates over time in a persistent memory store and is automatically surfaced when it becomes relevant, making every future conversation smarter and more personal than the last.</p>
        <p>Memory in ScalyClaw is not a list of chat logs. It is a structured, searchable store of extracted knowledge — discrete facts and observations that the LLM identifies and records during the course of normal conversation, then retrieves on demand without any user intervention.</p>

        <h2 id="how-it-works">How Memory Works</h2>

        <p>Memory operates entirely in the background. From the user's perspective, ScalyClaw simply seems to know things. Under the hood, two processes are running in every conversation: <strong>extraction</strong> and <strong>retrieval</strong>.</p>

        <h3>Extraction</h3>

        <p>During and after each conversation turn, the LLM evaluates the exchange for information worth preserving. This is not keyword matching or rule-based parsing — the model exercises judgment about what is meaningful, stable, and useful to remember. A user mentioning in passing that they work at Acme Corp is extracted as a <code>fact</code>. A user expressing frustration with verbose answers is extracted as a <code>preference</code>. Casual small talk that carries no durable information is discarded.</p>

        <p>When the LLM decides something is worth saving, it calls the built-in <code>memory_store</code> tool with a structured payload: the type, the content phrased as a clear, retrievable statement, a confidence score between 0 and 1 reflecting how certain the extraction is, and optional tags and TTL.</p>

        <h3>Storage</h3>

        <p>Memory entries are stored in SQLite using <code>bun:sqlite</code>, the high-performance native SQLite driver bundled with Bun. Each entry is stored with its text content alongside a float32 vector embedding generated by the configured embedding model. Two indexes are maintained in parallel:</p>

        <ul>
          <li><strong>sqlite-vec</strong> — a vector index for cosine similarity search. This is the primary retrieval path, used whenever an embedding model is configured and functional. It finds semantically similar memories even when the exact words do not match.</li>
          <li><strong>FTS5</strong> — SQLite's built-in full-text search index. This serves as the fallback when vector search is unavailable or returns insufficient results. It matches on exact words and common morphological variants.</li>
        </ul>

        <h3>Retrieval Flow</h3>

        <p>Every time a new message arrives, ScalyClaw runs a memory retrieval pass before constructing the system prompt for the LLM. The query used for retrieval is derived from the incoming message — or from a synthesis of the recent conversation context when more than one turn is available. The retrieval pipeline runs as follows:</p>

        <ol>
          <li>The query text is embedded using the configured embedding model to produce a float32 vector.</li>
          <li>sqlite-vec performs a cosine similarity search against all stored memory vectors, returning candidates ranked by distance.</li>
          <li>If sqlite-vec returns fewer than the required number of results — or if the embedding model is unavailable — FTS5 full-text search runs as a fallback against the same query.</li>
          <li>Results from both passes are merged, de-duplicated, and re-ranked by a combined score that weights cosine similarity and confidence together.</li>
          <li>The top-K results are formatted and injected into the system prompt as a <code>Memories</code> section, giving the LLM explicit access to what it knows about the user before it generates a response.</li>
        </ol>

        <div class="callout callout-info">
          <div class="callout-label">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><line x1="12" y1="16" x2="12" y2="12"/><line x1="12" y1="8" x2="12.01" y2="8"/></svg>
            Retrieval is non-blocking
          </div>
          <p>Memory retrieval runs concurrently with other pre-processing steps and adds negligible latency. The vector search over sqlite-vec completes in single-digit milliseconds for stores with tens of thousands of entries. The FTS5 fallback is similarly fast. Neither path involves a network call — all data is local.</p>
        </div>

        <h3>Retrieval Pipeline at a Glance</h3>

        <div class="code-block">
          <div class="code-block-header">
            <span class="code-block-lang">text</span>
            <button class="code-block-copy" aria-label="Copy"></button>
          </div>
          <pre><span class="cmt">Incoming message</span>
       <span class="op">↓</span>
<span class="fn">embed</span>(query)  <span class="cmt">→  float32 vector</span>
       <span class="op">↓</span>
<span class="fn">sqlite-vec cosine search</span>  <span class="cmt">→  ranked candidates</span>
       <span class="op">↓</span>  <span class="cmt">(if insufficient results)</span>
<span class="fn">FTS5 full-text fallback</span>   <span class="cmt">→  keyword candidates</span>
       <span class="op">↓</span>
<span class="fn">merge + deduplicate + re-rank</span>
  <span class="cmt">score = α × cosine_sim + β × confidence</span>
       <span class="op">↓</span>
<span class="fn">top-K results</span>  <span class="cmt">→  injected into system prompt</span></pre>
        </div>

        <h2 id="types">Memory Types</h2>

        <p>Every memory entry is tagged with a type that describes the nature of the information. The type informs how the LLM interprets and uses the memory, and it is also available as a filter in the dashboard memory browser.</p>

        <table>
          <thead>
            <tr><th>Type</th><th>Description</th><th>Example</th><th>Confidence pattern</th></tr>
          </thead>
          <tbody>
            <tr>
              <td><span class="badge badge-blue">fact</span></td>
              <td>An objective, verifiable piece of information about the user or their world. Facts are treated as stable — they do not expire on their own but can be updated when the user provides new information.</td>
              <td><em>"User works at Acme Corp."</em></td>
              <td>Starts high (0.85–1.0) when stated directly; lower (0.5–0.7) when inferred. Confidence rises on subsequent confirmation.</td>
            </tr>
            <tr>
              <td><span class="badge badge-green">preference</span></td>
              <td>A stated or observed inclination — how the user likes to receive information, what tools or formats they favor, what topics they want avoided. Preferences directly shape how the LLM responds.</td>
              <td><em>"Prefers concise answers over verbose explanations."</em></td>
              <td>Inferred preferences start around 0.6 and strengthen each time the preference is reinforced or explicitly confirmed.</td>
            </tr>
            <tr>
              <td><span class="badge badge-purple">relationship</span></td>
              <td>Information about people in the user's life — names, roles, and connections. Lets ScalyClaw refer to the user's colleagues, family, or friends by name without asking every time.</td>
              <td><em>"User's partner is named Alex."</em></td>
              <td>High confidence (0.9+) when stated directly. Moderate (0.6–0.8) when inferred from context such as pronoun use or indirect reference.</td>
            </tr>
            <tr>
              <td><span class="badge badge-amber">event</span></td>
              <td>Time-bound occurrences — appointments, deadlines, milestones, or past experiences the user has mentioned. Events are stored with whatever temporal context was provided.</td>
              <td><em>"User has a product review meeting on Friday."</em></td>
              <td>Typically high confidence (0.85+) when the user states the event explicitly. Confidence is not automatically reduced after the event date passes — the event remains as a historical record.</td>
            </tr>
            <tr>
              <td><span class="badge badge-rose">analysis</span></td>
              <td>Higher-level patterns and observations synthesized by the LLM from multiple exchanges — behavioral tendencies, recurring topics, or inferred characteristics. More speculative than facts but valuable for personalizing responses over time.</td>
              <td><em>"User tends to ask technical questions about Python and data engineering."</em></td>
              <td>Starts low (0.4–0.6) as an inference, rises with accumulating evidence. Analysis entries are updated in place rather than duplicated as the pattern strengthens.</td>
            </tr>
          </tbody>
        </table>

        <p>Confidence scores are not static. The LLM may update a memory's confidence upward when new information corroborates it, or downward when the user contradicts or corrects a stored entry. Contradicted entries are not automatically deleted — they are updated with lower confidence and a note about the correction, preserving the history.</p>

        <div class="callout callout-tip">
          <div class="callout-label">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M12 16v-4M12 8h.01"/></svg>
            Confidence in retrieval ranking
          </div>
          <p>When memories are ranked for injection into the system prompt, confidence acts as a multiplier on the relevance score. A memory that is highly relevant but has low confidence (e.g. an uncertain inference) will rank below a moderately relevant memory with high confidence. This means well-established facts are surfaced reliably, while speculative analysis appears only when it is also strongly relevant to the current query.</p>
        </div>

        <h2 id="tools">Memory Tools</h2>

        <p>The LLM has five built-in memory tools available at all times during conversation. These tools are called automatically — the user does not need to ask ScalyClaw to remember something, and the user does not need to know the tools exist. The LLM decides when to call them based on what it determines is worth preserving or retrieving.</p>

        <h3><code>memory_store</code></h3>

        <p>Saves a new memory entry to the store. The LLM calls this when it identifies information in the conversation that is worth preserving for future use. If an entry with very similar content already exists, the tool updates the existing entry and adjusts its confidence rather than creating a duplicate.</p>

        <div class="code-block">
          <div class="code-block-header">
            <span class="code-block-lang">json</span>
            <button class="code-block-copy" aria-label="Copy"></button>
          </div>
          <pre><span class="cmt">// Example tool call — storing a preference inferred from conversation</span>
{
  <span class="prop">"name"</span>: <span class="str">"memory_store"</span>,
  <span class="prop">"arguments"</span>: {
    <span class="prop">"content"</span>: <span class="str">"User prefers code examples in TypeScript over JavaScript."</span>,
    <span class="prop">"type"</span>: <span class="str">"preference"</span>,
    <span class="prop">"tags"</span>: [<span class="str">"coding"</span>, <span class="str">"typescript"</span>],
    <span class="prop">"confidence"</span>: <span class="num">0.85</span>
  }
}</pre>
        </div>

        <div class="code-block">
          <div class="code-block-header">
            <span class="code-block-lang">json</span>
            <button class="code-block-copy" aria-label="Copy"></button>
          </div>
          <pre><span class="cmt">// Example tool call — storing a fact stated explicitly by the user</span>
{
  <span class="prop">"name"</span>: <span class="str">"memory_store"</span>,
  <span class="prop">"arguments"</span>: {
    <span class="prop">"content"</span>: <span class="str">"User works at Acme Corp as a senior backend engineer."</span>,
    <span class="prop">"type"</span>: <span class="str">"fact"</span>,
    <span class="prop">"tags"</span>: [<span class="str">"work"</span>, <span class="str">"employment"</span>],
    <span class="prop">"confidence"</span>: <span class="num">1.0</span>
  }
}</pre>
        </div>

        <h3><code>memory_search</code></h3>

        <p>Queries the memory store on demand. While retrieval is automatic before every response, the LLM may call <code>memory_search</code> mid-conversation when the user asks a question that might be answered from memory, or when the LLM wants to double-check what it knows before making a claim.</p>

        <div class="code-block">
          <div class="code-block-header">
            <span class="code-block-lang">json</span>
            <button class="code-block-copy" aria-label="Copy"></button>
          </div>
          <pre><span class="cmt">// Example tool call — searching for information about the user's work situation</span>
{
  <span class="prop">"name"</span>: <span class="str">"memory_search"</span>,
  <span class="prop">"arguments"</span>: {
    <span class="prop">"query"</span>: <span class="str">"user employer job role company"</span>,
    <span class="prop">"limit"</span>: <span class="num">5</span>
  }
}</pre>
        </div>

        <div class="code-block">
          <div class="code-block-header">
            <span class="code-block-lang">json</span>
            <button class="code-block-copy" aria-label="Copy"></button>
          </div>
          <pre><span class="cmt">// Example tool response — results returned to the LLM</span>
{
  <span class="prop">"results"</span>: [
    {
      <span class="prop">"id"</span>: <span class="str">"mem_01j9x4kzb2f3"</span>,
      <span class="prop">"type"</span>: <span class="str">"fact"</span>,
      <span class="prop">"content"</span>: <span class="str">"User works at Acme Corp as a senior backend engineer."</span>,
      <span class="prop">"confidence"</span>: <span class="num">1.0</span>,
      <span class="prop">"score"</span>: <span class="num">0.94</span>,
      <span class="prop">"createdAt"</span>: <span class="str">"2026-01-15T09:22:41Z"</span>,
      <span class="prop">"updatedAt"</span>: <span class="str">"2026-02-03T14:08:17Z"</span>
    }
  ]
}</pre>
        </div>

        <h3>Tool Parameters</h3>

        <table>
          <thead>
            <tr><th>Tool</th><th>Parameter</th><th>Type</th><th>Description</th></tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="5"><code>memory_store</code></td>
              <td><code>content</code></td>
              <td><span class="badge badge-blue">string</span></td>
              <td>The memory statement. Should be a complete, self-contained sentence that would make sense without any surrounding context.</td>
            </tr>
            <tr>
              <td><code>type</code></td>
              <td><span class="badge badge-blue">string</span></td>
              <td>Memory type: <code>fact</code>, <code>preference</code>, <code>relationship</code>, <code>event</code>, or <code>analysis</code>.</td>
            </tr>
            <tr>
              <td><code>tags</code></td>
              <td><span class="badge badge-blue">string[]</span></td>
              <td>Array of topic strings for filtering and organization. Used by the dashboard memory browser.</td>
            </tr>
            <tr>
              <td><code>confidence</code></td>
              <td><span class="badge badge-blue">number</span></td>
              <td>Certainty score from 0.0 to 1.0. 1.0 means the user stated it directly; lower values represent inferences of varying certainty.</td>
            </tr>
            <tr>
              <td><code>ttl</code></td>
              <td><span class="badge badge-amber">optional</span></td>
              <td>Time-to-live in seconds. When set, the memory entry is automatically expired after this duration.</td>
            </tr>
            <tr>
              <td rowspan="2"><code>memory_search</code></td>
              <td><code>query</code></td>
              <td><span class="badge badge-blue">string</span></td>
              <td>Natural-language search query. Can be a phrase, a list of keywords, or a full sentence describing what to look for.</td>
            </tr>
            <tr>
              <td><code>limit</code></td>
              <td><span class="badge badge-amber">optional</span></td>
              <td>Maximum number of results to return. Defaults to 10.</td>
            </tr>
            <tr>
              <td rowspan="1"><code>memory_recall</code></td>
              <td><code>id</code> / <code>type</code> / <code>tags</code></td>
              <td><span class="badge badge-amber">optional</span></td>
              <td>Browse memories by a specific ID, by type, or by tag. Useful for structured lookups when the LLM already knows what category of memory it needs.</td>
            </tr>
            <tr>
              <td rowspan="1"><code>memory_update</code></td>
              <td><code>subject</code>, <code>content</code>, <code>tags</code>, <code>confidence</code></td>
              <td><span class="badge badge-amber">optional</span></td>
              <td>Update one or more fields of an existing memory entry. Only the fields provided are changed; the rest remain unchanged.</td>
            </tr>
            <tr>
              <td rowspan="1"><code>memory_delete</code></td>
              <td><code>id</code></td>
              <td><span class="badge badge-blue">string</span></td>
              <td>Permanently removes the memory entry with the given ID from the store.</td>
            </tr>
          </tbody>
        </table>

        <div class="callout callout-warn">
          <div class="callout-label">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M10.29 3.86L1.82 18a2 2 0 001.71 3h16.94a2 2 0 001.71-3L13.71 3.86a2 2 0 00-3.42 0z"/><line x1="12" y1="9" x2="12" y2="13"/><line x1="12" y1="17" x2="12.01" y2="17"/></svg>
            Memory content is permanent by default
          </div>
          <p>Memories are not automatically purged when they become stale. An event memory for "User has a meeting on Friday" will still exist after that Friday has passed — it becomes a historical record rather than an active reminder. The LLM is instructed to treat dated events with appropriate temporal awareness, but if you want guaranteed cleanup you should delete the entry manually via the dashboard or by telling ScalyClaw to forget it.</p>
        </div>

        <h2 id="management">Memory Management</h2>

        <p>The dashboard Memory page gives you full visibility into everything ScalyClaw has learned. It is the canonical interface for reviewing, editing, and pruning the memory store.</p>

        <h3>Browsing Memories</h3>

        <p>The memory browser displays all stored entries in a sortable, filterable list. Each row shows the memory content, its type badge, confidence score, and timestamps for when it was created and last updated. You can:</p>

        <ul>
          <li><strong>Search</strong> by keyword or phrase across all memory content — uses the same FTS5 index as the retrieval pipeline.</li>
          <li><strong>Filter by type</strong> to view only facts, preferences, relationships, events, or analysis entries.</li>
          <li><strong>Filter by confidence</strong> to surface high-confidence entries or to find uncertain inferences that may need review.</li>
          <li><strong>Sort</strong> by creation date, last-updated date, or confidence score.</li>
        </ul>

        <h3>Manual Operations</h3>

        <p>All memory operations available to the LLM are also available to you directly in the dashboard:</p>

        <table>
          <thead>
            <tr><th>Operation</th><th>Where</th><th>Description</th></tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Store new memory</strong></td>
              <td>Memory page &rarr; New entry</td>
              <td>Create a memory entry manually. Useful for bootstrapping — you can seed facts about yourself before any conversations take place. The entry is embedded and indexed immediately.</td>
            </tr>
            <tr>
              <td><strong>Edit entry</strong></td>
              <td>Memory page &rarr; Edit</td>
              <td>Modify the content, type, or confidence of an existing memory. The vector embedding is automatically regenerated from the updated content.</td>
            </tr>
            <tr>
              <td><strong>Delete entry</strong></td>
              <td>Memory page &rarr; Delete</td>
              <td>Permanently removes the memory from both the SQLite table and the vector index. This action cannot be undone from the dashboard — the entry is gone.</td>
            </tr>
            <tr>
              <td><strong>Bulk delete</strong></td>
              <td>Memory page &rarr; Select &rarr; Delete selected</td>
              <td>Select multiple entries with checkboxes and delete them in one action. Useful for cleaning up a batch of stale events or low-confidence inferences.</td>
            </tr>
            <tr>
              <td><strong>Re-embed all</strong></td>
              <td>Memory page &rarr; Settings &rarr; Re-embed all</td>
              <td>Regenerates vector embeddings for all entries using the currently configured embedding model. Required after changing embedding models. Can take several minutes for large stores.</td>
            </tr>
          </tbody>
        </table>

        <h3>Asking ScalyClaw to Forget</h3>

        <p>You do not need to visit the dashboard to remove a memory. If you tell ScalyClaw during any conversation "forget that I work at Acme Corp" or "please don't remember anything about my schedule", it will search for matching entries and call its internal <code>memory_delete</code> tool to remove them. The deletion takes effect immediately — the entries are gone from the store before the next conversation turn.</p>

        <div class="code-block">
          <div class="code-block-header">
            <span class="code-block-lang">json</span>
            <button class="code-block-copy" aria-label="Copy"></button>
          </div>
          <pre><span class="cmt">// Tool call issued by the LLM when user asks it to forget something</span>
{
  <span class="prop">"name"</span>: <span class="str">"memory_delete"</span>,
  <span class="prop">"arguments"</span>: {
    <span class="prop">"id"</span>: <span class="str">"mem_01j9x4kzb2f3"</span>
  }
}</pre>
        </div>

        <h3>Export and Backup</h3>

        <p>The memory store is an ordinary SQLite database file on disk. You can back it up, copy it, or inspect it with any SQLite browser. The dashboard also provides an export function that serializes all entries to a JSON array, suitable for archiving or migrating to another ScalyClaw instance.</p>

        <div class="callout callout-tip">
          <div class="callout-label">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M12 16v-4M12 8h.01"/></svg>
            Memories are shared across all channels
          </div>
          <p>The memory store is global, not per-channel. A fact learned during a Telegram conversation is immediately available in a Discord conversation, and vice versa. There is one unified knowledge base for the entire ScalyClaw instance. If you run multiple separate ScalyClaw deployments for different users, each deployment has its own independent memory store.</p>
        </div>

        <nav class="docs-page-nav">
          <a href="models.html" class="docs-page-nav-link">
            <span class="docs-page-nav-dir">Previous</span>
            <span class="docs-page-nav-title">Models &amp; Providers</span>
          </a>
          <a href="skills.html" class="docs-page-nav-link next">
            <span class="docs-page-nav-dir">Next</span>
            <span class="docs-page-nav-title">Skills</span>
          </a>
        </nav>
      </div>
    </main>
  </div>
  <footer class="docs-footer"><p>&copy; 2026 ScalyClaw</p></footer>
  <script src="docs.js"></script>
</body>
</html>
